# Phase 2: Online Learning - Implementation Started

## Summary

Phase 2 implementation has begun. We're building the 3-model ensemble with online learning (models update after each Claude forward).

## Architecture Clarified

### The 3 Models (All Binary Decisions + Online Learning)

**1. Router Model**
- Input: Query text (tokenized)
- Output: **Binary (0 or 1)** - forward or try local
- Architecture: Small transformer encoder (~500M params)
- Updates: After each forward (learn from success/failure)

**2. Generator Model**
- Input: Query text (tokenized)
- Output: **Response text** (generated tokens)
- Architecture: Medium transformer decoder (~7B params)
- Updates: After each forward (distillation from Claude)

**3. Validator Model**
- Input: Query + Generated response (tokenized)
- Output: **Binary (0 or 1)** - bad or good
- Architecture: Small transformer encoder (~500M params)
- Updates: After each forward (learn from quality assessment)

## Implementation Plan

### ‚úÖ Phase 1 Complete
- Basic proxy infrastructure
- Claude API client
- Metrics logging
- Template system (placeholder for Phase 2)

### üöß Phase 2 In Progress

**Done:**
- ‚úÖ Decided on Candle (Rust ML framework)
- ‚úÖ Added Candle dependencies (candle-core, candle-nn, candle-transformers, tokenizers, rand)
- ‚úÖ Created models module structure
- ‚úÖ Implemented Router model (binary classifier, ~290 lines, compiles successfully)
- ‚úÖ Implemented Generator model (autoregressive decoder, ~380 lines, compiles successfully)
- ‚úÖ Implemented Validator model (binary classifier, ~320 lines, compiles successfully)
- ‚úÖ Implemented Tokenizer module (~250 lines)
  - BPE tokenization with special tokens
  - encode/decode with Tensor support
  - Padding and truncation
  - Save/load from ~/.finch/tokenizer.json
- ‚úÖ Implemented Model Ensemble (~350 lines)
  - Coordinates all three models
  - Complete online learning training loop
  - Cold start strategy (0-50: always forward, 50-200: conservative)
  - Strategic sampling (50% ‚Üí 5% over time)
  - learn_from_claude() and learn_from_local_attempt() methods
  - Route decision and quality assessment
  - Model persistence (save/load all models)
- ‚úÖ Fixed all Candle API compilation errors
- ‚úÖ Created training framework design (docs/TRAINING_FRAMEWORK.md)
- ‚úÖ All components include comprehensive unit tests

**Next Steps:**

1. **Implement Proper Autograd/Backward Pass** ‚ö†Ô∏è CRITICAL BLOCKER
   - Candle doesn't have built-in autograd like PyTorch
   - Need to implement gradient computation manually
   - Current update() methods are placeholders
   - Options:
     - Implement manual backward passes for each layer
     - Use Candle's VarMap + SGD/Adam optimizer
     - Consider alternative training approach
   - This is the main blocker for actual training

2. **Semantic Similarity for Divergence Measurement**
   - Current: simple length-based placeholder
   - Need: proper semantic similarity (embeddings, cosine similarity)
   - Consider using sentence transformers or similar
   - Used to determine if local response is "good enough"

3. **Integration with Phase 1 Infrastructure**
   - Wire up ModelEnsemble to existing router module
   - Replace Phase 1 template matching with real model inference
   - Update CLI to initialize and save models
   - Connect to Claude API client for training

4. **Testing with Real Data**
   - Test forward passes with actual queries
   - Verify tokenization works correctly
   - Test routing decisions
   - Measure inference performance (latency, throughput)

5. **Model Persistence**
   - Implement proper model loading (currently unimplemented!())
   - Auto-save after N queries
   - Handle model versioning
   - Test save/load round-trip

## Key Decisions Made

### Pure Rust + Candle
- No Python dependencies
- Candle provides tensor ops, autograd, optimizers
- We implement custom model architectures
- Fast, native performance

### Binary Decisions (No Thresholds)
- Router outputs 0 or 1 directly (not confidence + threshold)
- Validator outputs 0 or 1 directly (not quality score)
- Simpler, models learn appropriate conservatism via training

### Random Initialization
- Models start with random/zero weights
- No pre-trained models to download
- Each user trains on their own data
- Fully personalized

### Online Learning (Not Batch Training)
- Update weights after EVERY forward to Claude
- No separate training phase
- Continuous improvement
- Day 1: random weights, forward 100%
- Day 30: decent models, forward 60%
- Day 180: good models, forward 5%

## File Structure

```
src/models/
‚îú‚îÄ‚îÄ mod.rs              # Module exports
‚îú‚îÄ‚îÄ common.rs           # Shared utilities, config, device selection (~50 lines)
‚îú‚îÄ‚îÄ router.rs           # Router model (binary classifier, ~290 lines)
‚îú‚îÄ‚îÄ generator.rs        # Generator model (text generation, ~380 lines)
‚îú‚îÄ‚îÄ validator.rs        # Validator model (binary classifier, ~320 lines)
‚îú‚îÄ‚îÄ tokenizer.rs        # Text tokenization (BPE, ~250 lines)
‚îî‚îÄ‚îÄ ensemble.rs         # Model ensemble + training loop (~350 lines)
```

Total: ~1,640 lines of Phase 2 model code

## Technical Stack

- **Framework:** Candle (Rust ML)
- **Models:** Custom transformers (implemented by us)
- **Optimization:** SGD/Adam with online updates
- **Tokenization:** BPE via `tokenizers` crate
- **Device:** Metal (Apple Silicon) or CPU
- **Persistence:** Save/load weights to `~/.finch/models/`

## Expected Timeline

- **Week 1:** Router model working + tests
- **Week 2:** Generator model working + tests
- **Week 3:** Validator model working + tests
- **Week 4:** Integration, online learning loop, testing
- **Week 5-6:** Bug fixes, optimization, documentation

## Current Blockers

1. **üî¥ CRITICAL: Gradient Computation** - Candle doesn't have PyTorch-style autograd. The `update()` methods in all three models are currently placeholders. Need to implement manual backward passes or find alternative approach.

2. **Model Loading** - Save is implemented but load is `unimplemented!()`. Need proper deserialization.

3. **Semantic Similarity** - Divergence measurement is a simple length-based placeholder. Need proper embeddings.

## Current Status Summary

**Infrastructure:** ‚úÖ COMPLETE
- All three models implemented and compiling
- Tokenization system working
- Model ensemble coordinating all components
- Training loop structure in place
- Cold start and sampling strategies implemented

**Training:** ‚ùå BLOCKED
- Forward passes work
- Backward passes are placeholders (no gradient computation)
- Cannot actually update weights yet
- This is the critical blocker

**Integration:** üü° READY
- Models can be integrated once training works
- Phase 1 infrastructure exists
- Just need to wire them together

## Next Immediate Steps

1. **Implement gradient computation** (CRITICAL)
   - Research Candle's training approach
   - Implement manual backward passes OR
   - Find alternative training method

2. **Test inference**
   - Test all three models with real queries
   - Verify tokenization correctness
   - Measure performance

3. **Implement model loading**
   - Complete the load() methods
   - Test save/load round-trip

4. **Wire up to Phase 1**
   - Replace template matching with ModelEnsemble
   - Update router to use models
   - Test end-to-end

---

**Status:** Phase 2 infrastructure COMPLETE (~1,640 lines). All models + tokenizer + ensemble implemented and compiling. BLOCKED on gradient computation for actual training.

**Last Updated:** 2026-01-29

**Lines of Code:** ~1,640 Phase 2 model code
